{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSMk5HWlU1FQ"
      },
      "outputs": [],
      "source": [
        "# 라이브러리 설치\n",
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZIad3PmU0mj"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRBcWUqGU3nZ"
      },
      "outputs": [],
      "source": [
        "GOOGLE_API_KEY = 'Gemini_API_KEY'\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtOptgkBU74Q"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZabNetiGb0E"
      },
      "source": [
        "### 텍스트 질문"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oz6kdHgU9aP"
      },
      "outputs": [],
      "source": [
        "# 질문과 답변\n",
        "request_str = input(\"Gemini에게 할 질문을 입력하세요: \")\n",
        "response = model.generate_conten(request_str)\n",
        "\n",
        "##sleep 이용 or 다른 방법으로 쉬게 만드셈\n",
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzameYZ-Gd4k"
      },
      "source": [
        "### 이미지 질문"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAP-0gT1VWfQ"
      },
      "outputs": [],
      "source": [
        "# 이미지\n",
        "import PIL.Image\n",
        "\n",
        "img = PIL.Image.open('image.jpg')\n",
        "model = genai.GenerativeModel('gemini-pro-vision')\n",
        "\n",
        "#이미지 넘겨주기\n",
        "response = model.generate_content(img)\n",
        "\n",
        "# 질문과 답변\n",
        "request_str = input(\"Gemini에게 할 질문을 입력하세요: \")\n",
        "response = model.generate_conten(request_str)\n",
        "\n",
        "##sleep 이용 or 다른 방법으로 쉬게 만드셈\n",
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZL-fLo3GhhT"
      },
      "source": [
        "### chat 형태"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij752aZqVoTW"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "chat = model.start_chat(history=[])\n",
        "chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOfAEhfBVqKG"
      },
      "outputs": [],
      "source": [
        "# 질문과 답변\n",
        "request_str = input(\"Gemini에게 할 질문을 입력하세요: \")\n",
        "response = chat.send_message(request_str)\n",
        "\n",
        "##sleep 이용 or 다른 방법으로 쉬게 만드문\n",
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgmhinYpV4Yy"
      },
      "outputs": [],
      "source": [
        "# history로 출력\n",
        "for message in chat.history:\n",
        "  display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPURE6odvXxO"
      },
      "source": [
        "# RAG 시도"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgLNxP3ULq0q",
        "outputId": "cb47dcf6-aabf-4ef3-be48-11f062cc402e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tk\n",
            "  Downloading tk-0.1.0-py3-none-any.whl.metadata (693 bytes)\n",
            "Downloading tk-0.1.0-py3-none-any.whl (3.9 kB)\n",
            "Installing collected packages: tk\n",
            "Successfully installed tk-0.1.0\n"
          ]
        }
      ],
      "source": [
        "#!pip install chromadb tk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxNaeM1qyPsV"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "import tk\n",
        "\n",
        "# Gemini API 키 설정\n",
        "genai.configure(api_key=\"\") ##키 넣어야함."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vySRlHdevZXU"
      },
      "outputs": [],
      "source": [
        "# ChromaDB 설정 및 데이터 저장\n",
        "def setup_chroma(documents):\n",
        "    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
        "    chroma_client = chromadb.Client()\n",
        "    collection = chroma_client.create_collection(name=\"my_collection4\", embedding_function=embedding_function)\n",
        "    collection.add(documents=documents, ids=[f\"id{i}\" for i in range(len(documents))])\n",
        "    return collection\n",
        "\n",
        "# 관련 문서 검색\n",
        "def retrieve_relevant_documents(query, collection, n_results=3):\n",
        "    results = collection.query(query_texts=[query], n_results=n_results)\n",
        "    return results[\"documents\"][0]\n",
        "\n",
        "# Gemini 채팅 모델 초기화\n",
        "def initialize_gemini_chat():\n",
        "    model = genai.GenerativeModel('gemini-1.5-pro')\n",
        "    chat = model.start_chat(history=[])\n",
        "    return chat\n",
        "\n",
        "# 답변 생성 및 채팅 기록 업데이트\n",
        "def throw_chat_context(chat, relevant_documents):\n",
        "    prompt = f\"\"\"\n",
        "    다음 context를 참고하여 질문에 답변하라.\n",
        "    문맥: {relevant_documents}\n",
        "    \"\"\"\n",
        "    response = chat.send_message(prompt)\n",
        "\n",
        "# 이미지 파일을 base64로 인코딩\n",
        "def encode_image(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "def generate_chat_response(chat, query, image_path=None):\n",
        "    prompt = f\"\"\"\n",
        "    chat의 context를 참고하여 질문에 답변하세요.\n",
        "    질문: {query}\n",
        "    답변:\n",
        "    \"\"\"\n",
        "    if image_path :\n",
        "        image_data = encode_image(image_path)\n",
        "        response = chat.send_message(\n",
        "            [prompt, {\"mime_type\":\"image/jpeg\", \"data\":image_data}]\n",
        "        )\n",
        "    else :\n",
        "        response = chat.send_message(prompt)\n",
        "    return response.text\n",
        "\n",
        "def select_image():\n",
        "    root = tk.Tk()\n",
        "    root.withdraw()\n",
        "    file_path = filedialog.askopenfilename(filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png\")])\n",
        "    return file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qpKCph4JyUxI",
        "outputId": "408ae0d9-d525-4417-81ca-c4ab4c9e029a"
      },
      "outputs": [],
      "source": [
        "# 예시 데이터-폴더에서 .txt 파일 읽어오기\n",
        "import os\n",
        "def load_documents_from_folder(folder_path):\n",
        "    documents = []\n",
        "    filenames = []\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):  # .txt 파일만 로드\n",
        "            with open(os.path.join(folder_path, filename), \"r\", encoding=\"utf-8\") as file:\n",
        "                content = file.read().strip()\n",
        "                documents.append(content)\n",
        "                filenames.append(filename)\n",
        "\n",
        "    return documents, filenames\n",
        "\n",
        "folder_path = \"./doc_folder\"\n",
        "documents = load_documents_from_folder(folder_path)\n",
        "\n",
        "# print(documents[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvqGSQcxM6DT"
      },
      "outputs": [],
      "source": [
        "# ChromaDB 설정\n",
        "collection = setup_chroma(documents[0])\n",
        "\n",
        "# Gemini 채팅 모델 초기화\n",
        "chat = initialize_gemini_chat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "VHXV3rgXyGp_",
        "outputId": "f0690ea2-2e4d-474f-8202-352a065318ef"
      },
      "outputs": [],
      "source": [
        "# 사용자 질의 및 답변 생성\n",
        "user_query = input(\"질문의 주제가 무엇인가요?: \")\n",
        "relevant_docs = retrieve_relevant_documents(user_query, collection)\n",
        "throw_chat_context(chat, relevant_docs)\n",
        "\n",
        "while True:\n",
        "    user_query = input(\"질문: \")\n",
        "    if user_query.lower() == \"종료\":\n",
        "        break\n",
        "    if user_query.lower() == \"이미지 입력\":\n",
        "        print(\"이미지 입력은 아직 미완성\")\n",
        "        '''\n",
        "        image_path = select_image()\n",
        "        if image_path:\n",
        "            user_query = input(\"질문: \")\n",
        "            answer = generate_chat_response(chat, user_query, image_path)\n",
        "        else:\n",
        "            print(\"이미지 선택이 취소되었습니다.\")\n",
        "            continue\n",
        "        '''\n",
        "    else:\n",
        "        answer = generate_chat_response(chat, user_query)\n",
        "    print(\"답변:\", answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "fFns0sfwNQ7Q",
        "outputId": "d74e6ff5-dbc9-4af8-bd33-753b30e0a140"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.5-pro-exp-03-25\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/learnlm-1.5-pro-experimental\n",
            "models/gemma-3-27b-it\n"
          ]
        }
      ],
      "source": [
        "# 모델 종류-예시 코드에서 업데이트가 있는 경우 바꿔야함.\n",
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PedsmhGmOCPK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
